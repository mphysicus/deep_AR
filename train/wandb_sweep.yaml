# Weights & Biases Sweep Configuration for DeepAR Hyperparameter Tuning
program: train_ddp.py
method: random
metric:
  name: val_loss
  goal: minimize
parameters:
  # --- Fixed Parameters for the Sweep ---
  model_type:
    value: "vit_h" 
  train_input_dir:
    value: "/storage/prakhar/Sem_Project/data_files/merged/train"
  val_input_dir:
    value: "/storage/prakhar/Sem_Project/data_files/merged/val"
  original_sam_checkpoint:
    value: "/storage/prakhar/Sem_Project/weights/original_weights/sam_vit_l.pth"
  peft_method:
    value: "lora"
  epochs:
    value: 50
  early_stopping_patience:
    value: 15
  use_wandb:
    value: True
  wandb_project:
    value: "deep_ar_vit_h" 

  # --- Hyperparameters to Tune ---
  
  initial_lr:
    value: 1e-4
    
  batch_size:
    values: [16, 32, 64]

  lora_rank:
    distribution: q_uniform
    min: 20
    max: 52
    q: 4 # Step size of 4 (e.g., 4, 8, 12...)

  lora_alpha:
    distribution: q_uniform
    min: 8
    max: 64
    q: 8 # Step size of 8 (e.g., 8, 16, 24...)

  lora_dropout:
    values: [0.0, 0.05, 0.1]